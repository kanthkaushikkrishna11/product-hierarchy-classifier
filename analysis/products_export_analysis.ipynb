{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from collections import Counter\n",
        "import re\n",
        "import logging\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def profile_dataset(filepath: str):\n",
        "    \"\"\"\n",
        "    Analyzes the product dataset to extract key patterns, distributions,\n",
        "    and potential data quality issues, printing the output directly.\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"      üöÄ STARTING PRODUCT DATASET PROFILE üöÄ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Assuming the file is in the root of your Colab environment\n",
        "        df = pd.read_csv(filepath)\n",
        "        total_rows = len(df)\n",
        "        print(f\"\\n‚úÖ Successfully loaded {total_rows} rows from '{filepath}'.\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ERROR: File not found at '{filepath}'.\")\n",
        "        print(\"Please make sure 'products-export.csv' is uploaded to your Colab session.\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Category and Sub-Category Distribution ---\n",
        "    print(\"\\n\" + \"=\"*20 + \" 1. Category Distribution \" + \"=\"*20)\n",
        "    print(df['category'].value_counts())\n",
        "    print(\"\\n\" + \"=\"*20 + \" 2. Sub-Category Distribution \" + \"=\"*20)\n",
        "    print(df['sub_category'].value_counts())\n",
        "\n",
        "    # --- 2. Brand Analysis ---\n",
        "    print(\"\\n\" + \"=\"*20 + \" 3. Brand Analysis \" + \"=\"*20)\n",
        "    brand_counts = df['brand'].str.lower().str.strip().value_counts()\n",
        "    print(\"\\nTotal List of  brand names from 'brand' column:\")\n",
        "    print(df['brand'].str.lower().str.strip().unique())\n",
        "    print(\"\\nTotal No of brands from 'brand' column:\")\n",
        "    print(df['brand'].str.lower().str.strip().nunique())\n",
        "    print(\"\\nTop 20 Brands from 'brand' column:\")\n",
        "    print(brand_counts.head(20))\n",
        "\n",
        "    print(\"\\nTop 50 Brands from 'brand' column:\")\n",
        "    print(brand_counts.head(50))\n",
        "\n",
        "    known_brands = ['apple', 'dell', 'hp', 'lenovo', 'asus', 'acer', 'microsoft',\n",
        "                    'samsung', 'lg', 'sony', 'toshiba', 'msi', 'razer', 'google',\n",
        "                    'hisense', 'tcl', 'vizio']\n",
        "\n",
        "    def guess_brand_from_name(name):\n",
        "        name_lower = str(name).lower()\n",
        "        for brand in known_brands:\n",
        "            if brand in name_lower:\n",
        "                return brand\n",
        "        return None\n",
        "\n",
        "    df['guessed_brand'] = df['name'].apply(guess_brand_from_name)\n",
        "    guessed_brand_counts = df[df['brand'].isna()]['guessed_brand'].value_counts()\n",
        "    print(\"\\n\\nBrands guessed from 'name' where 'brand' column is NULL:\")\n",
        "    print(guessed_brand_counts)\n",
        "\n",
        "    # --- 3. 'details' Column JSON Structure Analysis ---\n",
        "    print(\"\\n\" + \"=\"*20 + \" 4. 'details' JSON Analysis \" + \"=\"*20)\n",
        "    json_parse_errors = 0\n",
        "    spec_key_counter = Counter()\n",
        "\n",
        "    for details_str in df['details'].dropna():\n",
        "        try:\n",
        "            details_json = json.loads(details_str)\n",
        "            if 'specifications' in details_json and isinstance(details_json['specifications'], dict):\n",
        "                for key in details_json['specifications'].keys():\n",
        "                    spec_key_counter[key] += 1\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            json_parse_errors += 1\n",
        "\n",
        "    print(f\"\\nRows with JSON parsing errors in 'details': {json_parse_errors}\")\n",
        "    print(f\"Percentage of parse errors: {json_parse_errors / total_rows:.2%}\")\n",
        "    print(\"\\n\\nNo of keys in 'details.specifications':\")\n",
        "    print(len(spec_key_counter.items()))\n",
        "    print(\"\\n\\nTop 100 most common keys in 'details.specifications':\")\n",
        "    for key, count in spec_key_counter.most_common(100):\n",
        "        print(f\"- {key}: {count} times\")\n",
        "\n",
        "    # --- 4. Anomaly Detection in Key Specs ---\n",
        "    print(\"\\n\" + \"=\"*20 + \" 5. Potential Data Anomalies \" + \"=\"*20)\n",
        "\n",
        "    storage_anomalies = []\n",
        "    storage_pattern = re.compile(r'(\\d+\\.?\\d*)\\s*(TB|GB|MB)', re.IGNORECASE)\n",
        "    for text in df['name'].dropna():\n",
        "        matches = storage_pattern.findall(text)\n",
        "        for val, unit in matches:\n",
        "            try:\n",
        "                val_f = float(val)\n",
        "                if (unit.upper() == 'TB' and val_f > 10) or (unit.upper() == 'GB' and val_f > 8192):\n",
        "                     storage_anomalies.append(f\"Suspicious storage: '{val} {unit}' in title: '{text[:100]}...'\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    print(\"\\nSuspicious Storage Values (e.g., >10TB):\")\n",
        "    if storage_anomalies:\n",
        "        for anomaly in storage_anomalies[:10]:\n",
        "            print(f\"- {anomaly}\")\n",
        "    else:\n",
        "        print(\"None found in initial scan.\")\n",
        "\n",
        "    screen_size_anomalies = []\n",
        "    screen_pattern = re.compile(r'(\\d+\\.?\\d*)\\s*(?:inch|\")', re.IGNORECASE)\n",
        "    for text in df['name'].dropna():\n",
        "        matches = screen_pattern.findall(text)\n",
        "        for val in matches:\n",
        "            try:\n",
        "                val_f = float(val)\n",
        "                if not (10 <= val_f <= 100):\n",
        "                    screen_size_anomalies.append(f\"Unusual screen size: '{val}\\\"' in title: '{text[:100]}...'\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    print(\"\\n\\nSuspicious Screen Sizes (e.g., <10\\\" or >100\\\"):\")\n",
        "    print(f\"\\n\\n Length of screen_size_anomalies list:{len(screen_size_anomalies)}\")\n",
        "    if screen_size_anomalies:\n",
        "        for anomaly in screen_size_anomalies:\n",
        "            print(f\"- {anomaly}\")\n",
        "    else:\n",
        "        print(\"None found in initial scan.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"      üèÅ PROFILE COMPLETE üèÅ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# --- EXECUTE THE PROFILING ---\n",
        "# Ensure the path to your uploaded file is correct.\n",
        "# If you placed it in a sub-folder (e.g., 'data'), change the path.\n",
        "input_file_path = '/content/drive/MyDrive/Product_Hierarchy_Classifier/products-export.csv'\n",
        "profile_dataset(input_file_path)"
      ],
      "metadata": {
        "id": "Q1Kpg7cJFF0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FMpiHruGbc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}